{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_functionalities(clf, type_of_data, num_of_splits, bifurcation, metric=None):\n",
    "    '''\n",
    "    Input arguments:\n",
    "        clf = type of classifier\n",
    "        type_of_data = {'PEA', 'synthetic'}\n",
    "        num_of_splits = number of splits for cv\n",
    "        bifurcation = {'weighted','classwise'}\n",
    "        metric = {'draw_roc_auc'}\n",
    "    Return: X and y\n",
    "    \n",
    "    Info:\n",
    "        PEA X and y is returned as a dataframe\n",
    "        Synthetic X and y is returned as arrays\n",
    "    ********************************************************\n",
    "    '''\n",
    "    \n",
    "    list_values = list(np.round(np.arange(0, 1, 0.2),2))\n",
    "    \n",
    "    if type_of_data=='PEA' and  metric==None:\n",
    "        X, y = get_PEA_data()\n",
    "        print('PEA')\n",
    "        \n",
    "    elif (type_of_data=='PEA')and(metric=='draw_roc_auc'):\n",
    "        X, y = get_PEA_data()\n",
    "        print('draw_roc_auc')\n",
    "        plot_roc_auc(clf, X, y, num_of_splits)\n",
    "        \n",
    "    elif (type_of_data=='synthetic')and(metric=='draw_roc_auc'):\n",
    "        print('draw_roc_auc not valid with synthetic data')\n",
    "        \n",
    "    elif type_of_data=='synthetic':\n",
    "        pass\n",
    "            \n",
    "    #return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multilabel_CM(y_test, prediction):\n",
    "    \n",
    "    mcm = multilabel_confusion_matrix(y_test, prediction, labels=[0,1,2])\n",
    "    \n",
    "    tn = mcm[:, 0, 0]\n",
    "    tp = mcm[:, 1, 1]\n",
    "    fn = mcm[:, 1, 0]\n",
    "    fp = mcm[:, 0, 1]\n",
    "    \n",
    "    return tn, tp, fn, fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_classwise(clf, metric, class_label, folds):\n",
    "    \n",
    "    list_of_class_sensitivity = []\n",
    "    list_values = list(np.round(np.arange(0, 1, 0.2),2))\n",
    "    \n",
    "    for noise in list_values:\n",
    "        list_temp_A = []\n",
    "        list_temp_B = []\n",
    "        list_temp_C = []\n",
    "        X, y = helper_data_generator.make_classification(n_samples=5000,\n",
    "                                       n_features=26,\n",
    "                                       n_informative=21,\n",
    "                                       n_redundant=5,\n",
    "                                       n_repeated=0,\n",
    "                                       n_classes=3,\n",
    "                                       n_clusters_per_class=1,\n",
    "                                       class_sep=0.9,\n",
    "                                       flip_y=noise,\n",
    "                                       weights=[0.65, 0.01, 0.34])\n",
    "        \n",
    "#         column_size = X.shape[1]\n",
    "#         cols = [x + str(i) for x, i in zip([\"col\"]*column_size,\n",
    "#                                                range(column_size))]\n",
    "\n",
    "#         y = pd.DataFrame(y, columns=['label'])\n",
    "#         y.index = np.arange(0, len(y))\n",
    "#         y.index.name = 'index'\n",
    "\n",
    "#         syn_df = pd.DataFrame(X, columns=cols)\n",
    "#         syn_df.index = np.arange(0, len(syn_df))\n",
    "#         syn_df.index.name = 'index'\n",
    "#         syn_df = syn_df.merge(y, on='index')\n",
    "        #display(syn_df)\n",
    "\n",
    "        #skf = StratifiedKFold(n_splits=folds, shuffle=True)\n",
    "        skf = StratifiedShuffleSplit(n_splits=folds, test_size=0.2)\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            #display(len(np.unique(y_test)))\n",
    "            n_classes = len(np.unique(y_test))\n",
    "#         rkf = KFold(n_splits=folds)\n",
    "#         for train, test in rkf.split(syn_df):\n",
    "#             syn_df_train = syn_df.loc[train]\n",
    "#             syn_df_test = syn_df.loc[test]\n",
    "#             X_train = syn_df_train.loc[:, cols].reset_index(drop=True)\n",
    "#             y_train = syn_df_train.loc[:, 'label'].reset_index(drop=True)\n",
    "#             X_test = syn_df_test.loc[:, cols].reset_index(drop=True)\n",
    "#             y_test = syn_df_test.loc[:, 'label'].reset_index(drop=True)\n",
    "#             X_train, y_train = shuffle(X_train, y_train)\n",
    "            #print(clf)\n",
    "            clf.fit(X_train, y_train)\n",
    "            prediction = clf.predict(X_test)\n",
    "            \n",
    "            if metric=='roc_auc_metric':\n",
    "                y_test = label_binarize(y_test, classes=[0,1,2])\n",
    "                prediction = label_binarize(prediction, classes=[0,1,2])\n",
    "                fpr = dict()\n",
    "                tpr = dict()\n",
    "                roc_auc = dict()\n",
    "                # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "                for i in range(n_classes):\n",
    "                    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], prediction[:, i])\n",
    "                    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "                #print('roc_auc_class: ',roc_auc)\n",
    "\n",
    "                list_temp_A.append(roc_auc[0])\n",
    "                list_temp_B.append(roc_auc[1])\n",
    "                list_temp_C.append(roc_auc[2])\n",
    "            elif metric=='precision_recall':\n",
    "                y_test = label_binarize(y_test, classes=[0,1,2])\n",
    "                prediction = label_binarize(prediction, classes=[0,1,2])\n",
    "                precision = dict()\n",
    "                recall = dict()\n",
    "                pr_auc = dict()\n",
    "                # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "                for i in range(n_classes):\n",
    "                    precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], prediction[:, i])\n",
    "                    pr_auc[i] = auc(recall[i], precision[i])\n",
    "                #print('roc_auc_class: ',roc_auc)\n",
    "\n",
    "                list_temp_A.append(pr_auc[0])\n",
    "                list_temp_B.append(pr_auc[1])\n",
    "                list_temp_C.append(pr_auc[2])\n",
    "            else:\n",
    "                # tn, tp, fn, fp = get_multilabel_CM(y_test, prediction)\n",
    "                if metric=='fnr':\n",
    "                    fnr_class_A, fnr_class_B, fnr_class_C = get_scores(y_test, prediction, metric)\n",
    "                    list_temp_A.append(fnr_class_A)\n",
    "                    list_temp_B.append(fnr_class_B)\n",
    "                    list_temp_C.append(fnr_class_C)\n",
    "                elif metric=='fpr':\n",
    "                    fpr_class_A, fpr_class_B, fpr_class_C = get_scores(y_test, prediction, metric)\n",
    "                    list_temp_A.append(fpr_class_A)\n",
    "                    list_temp_B.append(fpr_class_B)\n",
    "                    list_temp_C.append(fpr_class_C)\n",
    "                elif metric=='sensitivity':\n",
    "                    sensitivity_recall_tpr_class_A, sensitivity_recall_tpr_class_B, sensitivity_recall_tpr_class_C = get_scores(metric, y_test, prediction)\n",
    "                    list_temp_A.append(sensitivity_recall_tpr_class_A)\n",
    "                    list_temp_B.append(sensitivity_recall_tpr_class_B)\n",
    "                    list_temp_C.append(sensitivity_recall_tpr_class_C)\n",
    "                elif metric=='specificity':\n",
    "                    speificity_neg_recall_tnr_class_A, speificity_neg_recall_tnr_class_B, speificity_neg_recall_tnr_class_C = get_scores(y_test, prediction, metric)\n",
    "                    list_temp_A.append(speificity_neg_recall_tnr_class_A)\n",
    "                    list_temp_B.append(speificity_neg_recall_tnr_class_B)\n",
    "                    list_temp_C.append(speificity_neg_recall_tnr_class_C)\n",
    "                \n",
    "        list_of_class_sensitivity.append((np.mean(list_temp_A),np.mean(list_temp_B),np.mean(list_temp_C)))\n",
    "    \n",
    "    if class_label=='0':\n",
    "        return list(np.round(list_of_class_sensitivity,2)[:,0]) # class A or -1\n",
    "    elif class_label=='1':\n",
    "        return list(np.round(list_of_class_sensitivity,2)[:,1]) # class A or -1\n",
    "    elif class_label=='2':\n",
    "        return list(np.round(list_of_class_sensitivity,2)[:,2]) # class A or -1\n",
    "    \n",
    "def create_dict_clf_noise_classwise(classifier_list, metric, class_label, folds):\n",
    "    \n",
    "    list_lists_clf = []\n",
    "    list_lists_clf_metric = []\n",
    "\n",
    "    for i in range(len(classifier_list)):\n",
    "        list_lists_clf.append('list'+str(i))\n",
    "\n",
    "    for i in range(len(list_lists_clf)):\n",
    "        list_lists_clf[i] = []\n",
    "    \n",
    "    print(metric)\n",
    "    for i in range(len(classifier_list)):\n",
    "        if isinstance(classifier_list[i],PerformanceEnrichmentAnalysisClassifier):\n",
    "            name = 'PEA'\n",
    "            list_lists_clf_metric.append((name,noise_classwise(classifier_list[i], metric, class_label, folds)))\n",
    "        else:\n",
    "            list_lists_clf_metric.append((str(classifier_list[i]).split('(')[0],noise_classwise(classifier_list[i], metric, class_label, folds)))\n",
    "            #list_lists_clf_metric.append((str(classifier_list[i]).split('(')[0],noise(classifier_list[i], metric, folds)))\n",
    "    \n",
    "    dict_items= {}\n",
    "\n",
    "    for i in range(np.array(list_lists_clf_metric).shape[0]):\n",
    "        dict_items[np.unique(np.array(list_lists_clf_metric).T[0][i])[0]] = np.hstack((np.array(list_lists_clf_metric).T[1][i]))\n",
    "\n",
    "    create_dataframe_classwise(dict_items, class_label, metric)\n",
    "    \n",
    "def create_dataframe_classwise(dict_items, class_label, metric):\n",
    "    \n",
    "    list_lists_clf = []\n",
    "\n",
    "    for i in range(5):\n",
    "        list_lists_clf.append('list'+str(i))\n",
    "\n",
    "    for i in range(5):\n",
    "        list_lists_clf[i] = []\n",
    "\n",
    "    for i in range(5):\n",
    "        for key, value in dict_items.items():\n",
    "            #print(key, value)\n",
    "            list_lists_clf[i].append((key,value[i]))\n",
    "        #print(len(value))\n",
    "        #print(np.array(dict_items['SVC'].T).reshape(-1,1))\n",
    "\n",
    "    #print(list_lists_clf)\n",
    "\n",
    "    list_lists_clf = np.array(list_lists_clf)\n",
    "    a = list_lists_clf.T[0].reshape(1,-1)\n",
    "    a = a[0]\n",
    "\n",
    "    _, idx = np.unique(a, return_index=True)\n",
    "    x = a[np.sort(idx)]\n",
    "\n",
    "    list_noise_vals = list(np.round(np.arange(0, 1, 0.2),2))\n",
    "    list_noise_vals = np.array(list_noise_vals)\n",
    "\n",
    "    y = list_lists_clf.T[1]\n",
    "\n",
    "    col_noise = []\n",
    "    for i in list_noise_vals:\n",
    "        col_noise.append(('noise_'+str(i)))\n",
    "        \n",
    "    plot_bar_classwise(x, y, col_noise, metric, class_label)\n",
    "    \n",
    "def plot_bar_classwise(x, y, col_noise, metric, class_label):\n",
    "    # https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
    "    # https://blog.finxter.com/matplotlib-legend/\n",
    "\n",
    "    df_1 = pd.DataFrame(y.T, index=col_noise, columns=x, dtype='float')\n",
    "    display(df_1)\n",
    "    ax = df_1.plot(kind='bar',figsize=(16, 7.5),width=0.8, cmap='jet', alpha=0.8)\n",
    "    ax.set_yticks(np.arange(0,1.1,0.1))\n",
    "    ax.minorticks_on()\n",
    "    ax.set_ylim(0,2)\n",
    "    ax.grid(which='major', linestyle='-', linewidth='0.3', color='green', alpha=0.7)\n",
    "    #ax.grid(which='minor', linestyle=':', linewidth='0.5', color='black')\n",
    "    ax.set_xticklabels(df_1.index,fontsize=18,rotation=0)\n",
    "    ax.set_facecolor('white')\n",
    "    ax.set_ylabel(str(metric),fontsize=18)\n",
    "    ax.set_title('Noise vs '+str(metric)+'_class_'+class_label,fontsize=18)\n",
    "\n",
    "    # facecolor='grey',\n",
    "    ax.legend(['PEA: Mean = '+ str(round(df_1.mean(),2)[0]) +'& SD = '+str(round(df_1.std(),2)[0]),\n",
    "               'KNeighborsClassifier: Mean = '+ str(round(df_1.mean(),2)[1]) +'& SD = '+str(round(df_1.std(),2)[1]),\n",
    "               'SVC: Mean = '+ str(round(df_1.mean(),2)[2]) +'& SD = '+str(round(df_1.std(),2)[2]),\n",
    "               'DecisionTreeClassifier: Mean = '+ str(round(df_1.mean(),2)[3]) +'& SD = '+str(round(df_1.std(),2)[3]),\n",
    "               'AdaBoostClassifier: Mean = '+ str(round(df_1.mean(),2)[4]) +'& SD = '+str(round(df_1.std(),2)[4])],\n",
    "               fontsize=12, framealpha=0.3, facecolor='grey', edgecolor='black', title= str(metric)+'_class_'+class_label+' - Central Tendency', title_fontsize=14, borderpad=.9)\n",
    "\n",
    "    # legend = ax.legend()\n",
    "    # frame = legend.get_frame()\n",
    "    # frame.set_color('grey')\n",
    "\n",
    "    labels = list(round(df_1.std(),2))\n",
    "    add_value_labels(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(y_test, prediction, metric):\n",
    "    \n",
    "#     list_temp_A = []\n",
    "#     list_temp_B = []\n",
    "#     list_temp_C = []\n",
    "    \n",
    "    tn, tp, fn, fp = get_multilabel_CM(y_test, prediction)\n",
    "    \n",
    "    score_class_A = []\n",
    "    score_class_B = []\n",
    "    score_class_C = []\n",
    "    \n",
    "    if metric=='fnr':\n",
    "        score_class_A = (fn / (fn + tp))[0]\n",
    "        score_class_B = (fn / (fn + tp))[1]\n",
    "        score_class_C = (fn / (fn + tp))[2]\n",
    "    elif metric=='fpr':\n",
    "        score_class_A = (fp / (tn + fp))[0]\n",
    "        score_class_B = (fp / (tn + fp))[1]\n",
    "        score_class_C = (fp / (tn + fp))[2]\n",
    "    elif metric=='sensitivity':\n",
    "        score_class_A = (tp / (tp + fn))[0]\n",
    "        score_class_B = (tp / (tp + fn))[1]\n",
    "        score_class_C = (tp / (tp + fn))[2]\n",
    "    elif metric=='specificity':\n",
    "        score_class_A = (tn / (tn + fp))[0]\n",
    "        score_class_B = (tn / (tn + fp))[1]\n",
    "        score_class_C = (tn / (tn + fp))[2]\n",
    "        \n",
    "    return score_class_A, score_class_B, score_class_C\n",
    "\n",
    "def process_function():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
